{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAMES: WARA LAURA \n",
    "       SUNNY KALSI\n",
    "COURSE: CS329 STATISTICS AND MACHINE LEARNING \n",
    "\n",
    "The following model rates movies based on what people commented about on twitter. \n",
    "It works based on a collection of data obtained from twitter developers, cleans the data and detects de level of polarity and subjectivity. \n",
    "Since this is an unsupervised method we do not have a training and test data. Instead we have tested the algorithm in two tests of data, and compared it with a popular rating website. \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We begin by importing pandas and numpy libraries.\n",
    "The very first step is to read our csv file that contains the data dowloaded from twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>ACCOUNT</th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-06 23:56:50</td>\n",
       "      <td>Akeila_small</td>\n",
       "      <td>b'I love Jonah! #Ozark'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-06 23:56:01</td>\n",
       "      <td>TVandDinners</td>\n",
       "      <td>b'I just yelled \\xe2\\x80\\x9ckiss him on the mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-06 23:55:49</td>\n",
       "      <td>Sd5182957013</td>\n",
       "      <td>b'@Rogers22L @CapiLady Ruth is my favorite! #O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-06 23:53:11</td>\n",
       "      <td>Miss_Cati</td>\n",
       "      <td>b'Love #Ozark and heard the S3 ending was craz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-06 23:51:29</td>\n",
       "      <td>reallyIceAgain</td>\n",
       "      <td>b'Darlene is bay shit crazy #Ozark'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2020-05-04 17:34:37</td>\n",
       "      <td>OtisOshow</td>\n",
       "      <td>b\"This weekend we went to find the #Ozark main...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2020-05-04 17:23:29</td>\n",
       "      <td>emleschh</td>\n",
       "      <td>b'has Jason Bateman always been this hot or am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2020-05-04 17:12:21</td>\n",
       "      <td>shona__ann</td>\n",
       "      <td>b'Just finished #Ozark and I have no fucking c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2020-05-04 17:04:53</td>\n",
       "      <td>Daily_Express</td>\n",
       "      <td>b'Why is #Ozark so blue? #Ozarkseason3 #OzarkS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2020-05-04 17:04:39</td>\n",
       "      <td>bdebullshit</td>\n",
       "      <td>b'WENDY LENDARIAAAAAAA #Ozark'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    DATE         ACCOUNT  \\\n",
       "0    2020-05-06 23:56:50    Akeila_small   \n",
       "1    2020-05-06 23:56:01    TVandDinners   \n",
       "2    2020-05-06 23:55:49    Sd5182957013   \n",
       "3    2020-05-06 23:53:11       Miss_Cati   \n",
       "4    2020-05-06 23:51:29  reallyIceAgain   \n",
       "..                   ...             ...   \n",
       "995  2020-05-04 17:34:37       OtisOshow   \n",
       "996  2020-05-04 17:23:29        emleschh   \n",
       "997  2020-05-04 17:12:21      shona__ann   \n",
       "998  2020-05-04 17:04:53   Daily_Express   \n",
       "999  2020-05-04 17:04:39     bdebullshit   \n",
       "\n",
       "                                            Transcript  \n",
       "0                              b'I love Jonah! #Ozark'  \n",
       "1    b'I just yelled \\xe2\\x80\\x9ckiss him on the mo...  \n",
       "2    b'@Rogers22L @CapiLady Ruth is my favorite! #O...  \n",
       "3    b'Love #Ozark and heard the S3 ending was craz...  \n",
       "4                  b'Darlene is bay shit crazy #Ozark'  \n",
       "..                                                 ...  \n",
       "995  b\"This weekend we went to find the #Ozark main...  \n",
       "996  b'has Jason Bateman always been this hot or am...  \n",
       "997  b'Just finished #Ozark and I have no fucking c...  \n",
       "998  b'Why is #Ozark so blue? #Ozarkseason3 #OzarkS...  \n",
       "999                     b'WENDY LENDARIAAAAAAA #Ozark'  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#data = pd.read_csv('thelastKingdom.csv')\n",
    "#data = pd.read_csv('tigerKing.csv')\n",
    "#data = pd.read_csv('extraction_result.csv')\n",
    "#data = pd.read_csv('upload.csv')\n",
    "data = pd.read_csv('Ozark.csv')\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dropp all the empty rows that did not contain any useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>ACCOUNT</th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-06 23:56:50</td>\n",
       "      <td>Akeila_small</td>\n",
       "      <td>b'I love Jonah! #Ozark'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-06 23:56:01</td>\n",
       "      <td>TVandDinners</td>\n",
       "      <td>b'I just yelled \\xe2\\x80\\x9ckiss him on the mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-06 23:55:49</td>\n",
       "      <td>Sd5182957013</td>\n",
       "      <td>b'@Rogers22L @CapiLady Ruth is my favorite! #O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-06 23:53:11</td>\n",
       "      <td>Miss_Cati</td>\n",
       "      <td>b'Love #Ozark and heard the S3 ending was craz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-06 23:51:29</td>\n",
       "      <td>reallyIceAgain</td>\n",
       "      <td>b'Darlene is bay shit crazy #Ozark'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2020-05-04 17:34:37</td>\n",
       "      <td>OtisOshow</td>\n",
       "      <td>b\"This weekend we went to find the #Ozark main...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2020-05-04 17:23:29</td>\n",
       "      <td>emleschh</td>\n",
       "      <td>b'has Jason Bateman always been this hot or am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2020-05-04 17:12:21</td>\n",
       "      <td>shona__ann</td>\n",
       "      <td>b'Just finished #Ozark and I have no fucking c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2020-05-04 17:04:53</td>\n",
       "      <td>Daily_Express</td>\n",
       "      <td>b'Why is #Ozark so blue? #Ozarkseason3 #OzarkS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2020-05-04 17:04:39</td>\n",
       "      <td>bdebullshit</td>\n",
       "      <td>b'WENDY LENDARIAAAAAAA #Ozark'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    DATE         ACCOUNT  \\\n",
       "0    2020-05-06 23:56:50    Akeila_small   \n",
       "1    2020-05-06 23:56:01    TVandDinners   \n",
       "2    2020-05-06 23:55:49    Sd5182957013   \n",
       "3    2020-05-06 23:53:11       Miss_Cati   \n",
       "4    2020-05-06 23:51:29  reallyIceAgain   \n",
       "..                   ...             ...   \n",
       "995  2020-05-04 17:34:37       OtisOshow   \n",
       "996  2020-05-04 17:23:29        emleschh   \n",
       "997  2020-05-04 17:12:21      shona__ann   \n",
       "998  2020-05-04 17:04:53   Daily_Express   \n",
       "999  2020-05-04 17:04:39     bdebullshit   \n",
       "\n",
       "                                            Transcript  \n",
       "0                              b'I love Jonah! #Ozark'  \n",
       "1    b'I just yelled \\xe2\\x80\\x9ckiss him on the mo...  \n",
       "2    b'@Rogers22L @CapiLady Ruth is my favorite! #O...  \n",
       "3    b'Love #Ozark and heard the S3 ending was craz...  \n",
       "4                  b'Darlene is bay shit crazy #Ozark'  \n",
       "..                                                 ...  \n",
       "995  b\"This weekend we went to find the #Ozark main...  \n",
       "996  b'has Jason Bateman always been this hot or am...  \n",
       "997  b'Just finished #Ozark and I have no fucking c...  \n",
       "998  b'Why is #Ozark so blue? #Ozarkseason3 #OzarkS...  \n",
       "999                     b'WENDY LENDARIAAAAAAA #Ozark'  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to clean the data. \n",
    "We clean the data by elminating unwanted information, and putting all the text in the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = text[1:]\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love jonah ozark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i just yelled  him on the  to my empty living ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>capilady ruth is my favorite ozark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love ozark and heard the  ending was crazy tbh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>darlene is bay shit crazy ozark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>this weekend we went to find the ozark main ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>has jason bateman always been this hot or am i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>just finished ozark and i have no fucking clue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>why is ozark so blue   ozark wendy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>wendy lendariaaaaaaa ozark</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Transcript\n",
       "0                                   i love jonah ozark\n",
       "1    i just yelled  him on the  to my empty living ...\n",
       "2                   capilady ruth is my favorite ozark\n",
       "3    love ozark and heard the  ending was crazy tbh...\n",
       "4                      darlene is bay shit crazy ozark\n",
       "..                                                 ...\n",
       "995  this weekend we went to find the ozark main ho...\n",
       "996  has jason bateman always been this hot or am i...\n",
       "997  just finished ozark and i have no fucking clue...\n",
       "998               why is ozark so blue   ozark wendy  \n",
       "999                         wendy lendariaaaaaaa ozark\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = pd.DataFrame(data.Transcript.apply(round1))\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.to_pickle(\"extraction_corpus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep cleaning the data we apply the split() function to the dataser and store the count of the most common words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "common = Counter(\" \".join(data_clean[\"Transcript\"]).split()).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ozark</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>to</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a</td>\n",
       "      <td>275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>and</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>season</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>on</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>just</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>that</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>it</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>so</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>was</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>this</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>you</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>for</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>my</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word  count\n",
       "0    ozark   1035\n",
       "1      the    496\n",
       "2       of    325\n",
       "3        i    321\n",
       "4       is    318\n",
       "5       to    287\n",
       "6        a    275\n",
       "7      and    266\n",
       "8   season    219\n",
       "9       in    167\n",
       "10      on    160\n",
       "11    just    155\n",
       "12    that    123\n",
       "13      it    119\n",
       "14      so    112\n",
       "15     was    110\n",
       "16    this    108\n",
       "17     you    108\n",
       "18     for    102\n",
       "19      my     99"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(common)\n",
    "words = df1.rename(columns={0:'Word',1:'count'})\n",
    "words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To be able to Stem the data we will join all the words into a string named pat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "pat = '|'.join([r'\\b{}\\b'.format(w) for w in words['Word']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data together into a string we apply the nltk.stem wich \n",
    "will put every word into its base word. \n",
    "Following we clean the data one more time removing the 100 most commonly used words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "porter_stemmer = PorterStemmer()\n",
    "'''\n",
    "Write a function to perform the pre processing steps on the entire dataset\n",
    "'''\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stem_sentences(sentence):\n",
    "    tokens = sentence.split()\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "def clean_text_round2(text):\n",
    "    '''remove 100 most commonly used words'''\n",
    "    text = re.sub(pat, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we apply the clean_text_round2 function which removes the most repeated words. We also apply stem_sentences function which puts everyword to its base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transcript</th>\n",
       "      <th>trimmed transcript</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love jonah ozark</td>\n",
       "      <td>love jonah</td>\n",
       "      <td>love jonah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i just yelled  him on the  to my empty living ...</td>\n",
       "      <td>yelled  him      empty living room because  ...</td>\n",
       "      <td>yell him empti live room becaus am who am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>capilady ruth is my favorite ozark</td>\n",
       "      <td>capilady ruth   favorite</td>\n",
       "      <td>capiladi ruth favorit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love ozark and heard the  ending was crazy tbh...</td>\n",
       "      <td>love   heard   ending  crazy tbh not  crazy ja...</td>\n",
       "      <td>love heard end crazi tbh not crazi jake predic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>darlene is bay shit crazy ozark</td>\n",
       "      <td>darlene  bay shit crazy</td>\n",
       "      <td>darlen bay shit crazi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>this weekend we went to find the ozark main ho...</td>\n",
       "      <td>weekend we went  find   main home   byrds fro...</td>\n",
       "      <td>weekend we went find main home byrd from netfl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>has jason bateman always been this hot or am i...</td>\n",
       "      <td>has jason bateman always been  hot or am   att...</td>\n",
       "      <td>ha jason bateman alway been hot or am attract ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>just finished ozark and i have no fucking clue...</td>\n",
       "      <td>finished    have no fucking clue what  going ...</td>\n",
       "      <td>finish have no fuck clue what go through whole...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>why is ozark so blue   ozark wendy</td>\n",
       "      <td>why    blue    wendy</td>\n",
       "      <td>whi blue wendi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>wendy lendariaaaaaaa ozark</td>\n",
       "      <td>wendy lendariaaaaaaa</td>\n",
       "      <td>wendi lendariaaaaaaa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Transcript  \\\n",
       "0                                   i love jonah ozark   \n",
       "1    i just yelled  him on the  to my empty living ...   \n",
       "2                   capilady ruth is my favorite ozark   \n",
       "3    love ozark and heard the  ending was crazy tbh...   \n",
       "4                      darlene is bay shit crazy ozark   \n",
       "..                                                 ...   \n",
       "995  this weekend we went to find the ozark main ho...   \n",
       "996  has jason bateman always been this hot or am i...   \n",
       "997  just finished ozark and i have no fucking clue...   \n",
       "998               why is ozark so blue   ozark wendy     \n",
       "999                         wendy lendariaaaaaaa ozark   \n",
       "\n",
       "                                    trimmed transcript  \\\n",
       "0                                          love jonah    \n",
       "1      yelled  him      empty living room because  ...   \n",
       "2                            capilady ruth   favorite    \n",
       "3    love   heard   ending  crazy tbh not  crazy ja...   \n",
       "4                             darlene  bay shit crazy    \n",
       "..                                                 ...   \n",
       "995   weekend we went  find   main home   byrds fro...   \n",
       "996  has jason bateman always been  hot or am   att...   \n",
       "997   finished    have no fucking clue what  going ...   \n",
       "998                             why    blue    wendy     \n",
       "999                              wendy lendariaaaaaaa    \n",
       "\n",
       "                                               stemmed  \n",
       "0                                           love jonah  \n",
       "1            yell him empti live room becaus am who am  \n",
       "2                                capiladi ruth favorit  \n",
       "3    love heard end crazi tbh not crazi jake predic...  \n",
       "4                                darlen bay shit crazi  \n",
       "..                                                 ...  \n",
       "995  weekend we went find main home byrd from netfl...  \n",
       "996  ha jason bateman alway been hot or am attract ...  \n",
       "997  finish have no fuck clue what go through whole...  \n",
       "998                                     whi blue wendi  \n",
       "999                               wendi lendariaaaaaaa  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean['trimmed transcript'] = pd.DataFrame(data_clean.Transcript.apply(round2))\n",
    "data_clean['stemmed'] = data_clean['trimmed transcript'].apply(stem_sentences)\n",
    "data_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now find the polarity and subjectivity of the already cleaned data using the textblob library and add the columns to the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transcript</th>\n",
       "      <th>trimmed transcript</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>polarity</th>\n",
       "      <th>stemmed_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love jonah ozark</td>\n",
       "      <td>love jonah</td>\n",
       "      <td>love jonah</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i just yelled  him on the  to my empty living ...</td>\n",
       "      <td>yelled  him      empty living room because  ...</td>\n",
       "      <td>yell him empti live room becaus am who am</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>capilady ruth is my favorite ozark</td>\n",
       "      <td>capilady ruth   favorite</td>\n",
       "      <td>capiladi ruth favorit</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love ozark and heard the  ending was crazy tbh...</td>\n",
       "      <td>love   heard   ending  crazy tbh not  crazy ja...</td>\n",
       "      <td>love heard end crazi tbh not crazi jake predic...</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>0.262500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>darlene is bay shit crazy ozark</td>\n",
       "      <td>darlene  bay shit crazy</td>\n",
       "      <td>darlen bay shit crazi</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>this weekend we went to find the ozark main ho...</td>\n",
       "      <td>weekend we went  find   main home   byrds fro...</td>\n",
       "      <td>weekend we went find main home byrd from netfl...</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>has jason bateman always been this hot or am i...</td>\n",
       "      <td>has jason bateman always been  hot or am   att...</td>\n",
       "      <td>ha jason bateman alway been hot or am attract ...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>just finished ozark and i have no fucking clue...</td>\n",
       "      <td>finished    have no fucking clue what  going ...</td>\n",
       "      <td>finish have no fuck clue what go through whole...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>why is ozark so blue   ozark wendy</td>\n",
       "      <td>why    blue    wendy</td>\n",
       "      <td>whi blue wendi</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>wendy lendariaaaaaaa ozark</td>\n",
       "      <td>wendy lendariaaaaaaa</td>\n",
       "      <td>wendi lendariaaaaaaa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Transcript  \\\n",
       "0                                   i love jonah ozark   \n",
       "1    i just yelled  him on the  to my empty living ...   \n",
       "2                   capilady ruth is my favorite ozark   \n",
       "3    love ozark and heard the  ending was crazy tbh...   \n",
       "4                      darlene is bay shit crazy ozark   \n",
       "..                                                 ...   \n",
       "995  this weekend we went to find the ozark main ho...   \n",
       "996  has jason bateman always been this hot or am i...   \n",
       "997  just finished ozark and i have no fucking clue...   \n",
       "998               why is ozark so blue   ozark wendy     \n",
       "999                         wendy lendariaaaaaaa ozark   \n",
       "\n",
       "                                    trimmed transcript  \\\n",
       "0                                          love jonah    \n",
       "1      yelled  him      empty living room because  ...   \n",
       "2                            capilady ruth   favorite    \n",
       "3    love   heard   ending  crazy tbh not  crazy ja...   \n",
       "4                             darlene  bay shit crazy    \n",
       "..                                                 ...   \n",
       "995   weekend we went  find   main home   byrds fro...   \n",
       "996  has jason bateman always been  hot or am   att...   \n",
       "997   finished    have no fucking clue what  going ...   \n",
       "998                             why    blue    wendy     \n",
       "999                              wendy lendariaaaaaaa    \n",
       "\n",
       "                                               stemmed  polarity  \\\n",
       "0                                           love jonah  0.500000   \n",
       "1            yell him empti live room becaus am who am -0.100000   \n",
       "2                                capiladi ruth favorit  0.500000   \n",
       "3    love heard end crazi tbh not crazi jake predic... -0.025000   \n",
       "4                                darlen bay shit crazi -0.400000   \n",
       "..                                                 ...       ...   \n",
       "995  weekend we went find main home byrd from netfl...  0.013889   \n",
       "996  ha jason bateman alway been hot or am attract ...  0.250000   \n",
       "997  finish have no fuck clue what go through whole...  0.250000   \n",
       "998                                     whi blue wendi  0.000000   \n",
       "999                               wendi lendariaaaaaaa  0.000000   \n",
       "\n",
       "     stemmed_polarity  \n",
       "0            0.500000  \n",
       "1            0.136364  \n",
       "2            0.000000  \n",
       "3            0.262500  \n",
       "4           -0.200000  \n",
       "..                ...  \n",
       "995          0.020833  \n",
       "996          0.250000  \n",
       "997          0.200000  \n",
       "998          0.000000  \n",
       "999          0.000000  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create quick lambda functions to find the polarity and subjectivity of each routine\n",
    "# Terminal / Anaconda Navigator: conda install -c conda-forge textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "data_clean['polarity'] = data_clean['Transcript'].apply(pol)\n",
    "data_clean['stemmed_polarity'] = data_clean['stemmed'].apply(pol)\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we help the classfication adding 0.2 to base polarity and steemed polarity to not have any values out of range. \n",
    "We add two more columns in this section; Predicted Stars and Steemed Predicted Stars, which shows the star rating according to the polarity of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add 0.2 to base polarity and stemmed polarity to help with positive words not being classified right\n",
    "data_clean.polarity = data_clean.polarity+.2\n",
    "data_clean.stemmed_polarity = data_clean.stemmed_polarity +0.2\n",
    "data_clean['predicted_stars'] = 0\n",
    "data_clean['stemmed_predicted_stars'] = 0\n",
    "data_clean.loc[(data_clean.polarity <-0.8),'predicted_stars']=1\n",
    "data_clean.loc[(data_clean.stemmed_polarity <-0.8 ),'stemmed_predicted_stars']=1\n",
    "data_clean.loc[(data_clean.polarity >= -0.8) & (data_clean.polarity < -0.6),'predicted_stars']=2\n",
    "data_clean.loc[(data_clean.stemmed_polarity >= -0.8) & (data_clean.stemmed_polarity <-0.6 ),'stemmed_predicted_stars']=2\n",
    "data_clean.loc[(data_clean.polarity >= -0.6) & (data_clean.polarity < -0.4),'predicted_stars']=3\n",
    "data_clean.loc[(data_clean.stemmed_polarity >= -0.6) & (data_clean.stemmed_polarity <-0.4 ),'stemmed_predicted_stars']=3\n",
    "data_clean.loc[(data_clean.polarity >= -0.4) & (data_clean.polarity < -0.2),'predicted_stars']=4\n",
    "data_clean.loc[(data_clean.stemmed_polarity >= 0.4) & (data_clean.stemmed_polarity < -0.2 ),'stemmed_predicted_stars']=4\n",
    "data_clean.loc[(data_clean.polarity >= -0.2) & (data_clean.polarity < 0),'predicted_stars']=5\n",
    "data_clean.loc[(data_clean.stemmed_polarity >= -0.2) & (data_clean.stemmed_polarity <0 ),'stemmed_predicted_stars']=5\n",
    "data_clean.loc[(data_clean.polarity >= 0) & (data_clean.polarity < 0.2),'predicted_stars']=6\n",
    "data_clean.loc[(data_clean.stemmed_polarity >= 0) & (data_clean.stemmed_polarity <0.2 ),'stemmed_predicted_stars']=6\n",
    "data_clean.loc[(data_clean.polarity >= 0.2) & (data_clean.polarity < 0.4),'predicted_stars']=7\n",
    "data_clean.loc[(data_clean.stemmed_polarity >= 0.2) & (data_clean.stemmed_polarity <0.4 ),'stemmed_predicted_stars']=7\n",
    "data_clean.loc[(data_clean.polarity >= 0.4) & (data_clean.polarity < 0.6),'predicted_stars']=8\n",
    "data_clean.loc[(data_clean.stemmed_polarity >= 0.4) & (data_clean.stemmed_polarity <0.6),'stemmed_predicted_stars']=8\n",
    "data_clean.loc[(data_clean.polarity >= 0.6) & (data_clean.polarity < 0.8),'predicted_stars']=9\n",
    "data_clean.loc[(data_clean.stemmed_polarity >= 0.6) & (data_clean.stemmed_polarity <0.8 ),'stemmed_predicted_stars']=9\n",
    "data_clean.loc[(data_clean.polarity >= 0.8),'predicted_stars']=10\n",
    "data_clean.loc[(data_clean.stemmed_polarity >= 0.8),'stemmed_predicted_stars']=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transcript</th>\n",
       "      <th>trimmed transcript</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>polarity</th>\n",
       "      <th>stemmed_polarity</th>\n",
       "      <th>predicted_stars</th>\n",
       "      <th>stemmed_predicted_stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i love jonah ozark</td>\n",
       "      <td>love jonah</td>\n",
       "      <td>love jonah</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i just yelled  him on the  to my empty living ...</td>\n",
       "      <td>yelled  him      empty living room because  ...</td>\n",
       "      <td>yell him empti live room becaus am who am</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.336364</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>capilady ruth is my favorite ozark</td>\n",
       "      <td>capilady ruth   favorite</td>\n",
       "      <td>capiladi ruth favorit</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love ozark and heard the  ending was crazy tbh...</td>\n",
       "      <td>love   heard   ending  crazy tbh not  crazy ja...</td>\n",
       "      <td>love heard end crazi tbh not crazi jake predic...</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.462500</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>darlene is bay shit crazy ozark</td>\n",
       "      <td>darlene  bay shit crazy</td>\n",
       "      <td>darlen bay shit crazi</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>this weekend we went to find the ozark main ho...</td>\n",
       "      <td>weekend we went  find   main home   byrds fro...</td>\n",
       "      <td>weekend we went find main home byrd from netfl...</td>\n",
       "      <td>0.213889</td>\n",
       "      <td>0.220833</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>has jason bateman always been this hot or am i...</td>\n",
       "      <td>has jason bateman always been  hot or am   att...</td>\n",
       "      <td>ha jason bateman alway been hot or am attract ...</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>just finished ozark and i have no fucking clue...</td>\n",
       "      <td>finished    have no fucking clue what  going ...</td>\n",
       "      <td>finish have no fuck clue what go through whole...</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>why is ozark so blue   ozark wendy</td>\n",
       "      <td>why    blue    wendy</td>\n",
       "      <td>whi blue wendi</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>wendy lendariaaaaaaa ozark</td>\n",
       "      <td>wendy lendariaaaaaaa</td>\n",
       "      <td>wendi lendariaaaaaaa</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Transcript  \\\n",
       "0                                   i love jonah ozark   \n",
       "1    i just yelled  him on the  to my empty living ...   \n",
       "2                   capilady ruth is my favorite ozark   \n",
       "3    love ozark and heard the  ending was crazy tbh...   \n",
       "4                      darlene is bay shit crazy ozark   \n",
       "..                                                 ...   \n",
       "995  this weekend we went to find the ozark main ho...   \n",
       "996  has jason bateman always been this hot or am i...   \n",
       "997  just finished ozark and i have no fucking clue...   \n",
       "998               why is ozark so blue   ozark wendy     \n",
       "999                         wendy lendariaaaaaaa ozark   \n",
       "\n",
       "                                    trimmed transcript  \\\n",
       "0                                          love jonah    \n",
       "1      yelled  him      empty living room because  ...   \n",
       "2                            capilady ruth   favorite    \n",
       "3    love   heard   ending  crazy tbh not  crazy ja...   \n",
       "4                             darlene  bay shit crazy    \n",
       "..                                                 ...   \n",
       "995   weekend we went  find   main home   byrds fro...   \n",
       "996  has jason bateman always been  hot or am   att...   \n",
       "997   finished    have no fucking clue what  going ...   \n",
       "998                             why    blue    wendy     \n",
       "999                              wendy lendariaaaaaaa    \n",
       "\n",
       "                                               stemmed  polarity  \\\n",
       "0                                           love jonah  0.700000   \n",
       "1            yell him empti live room becaus am who am  0.100000   \n",
       "2                                capiladi ruth favorit  0.700000   \n",
       "3    love heard end crazi tbh not crazi jake predic...  0.175000   \n",
       "4                                darlen bay shit crazi -0.200000   \n",
       "..                                                 ...       ...   \n",
       "995  weekend we went find main home byrd from netfl...  0.213889   \n",
       "996  ha jason bateman alway been hot or am attract ...  0.450000   \n",
       "997  finish have no fuck clue what go through whole...  0.450000   \n",
       "998                                     whi blue wendi  0.200000   \n",
       "999                               wendi lendariaaaaaaa  0.200000   \n",
       "\n",
       "     stemmed_polarity  predicted_stars  stemmed_predicted_stars  \n",
       "0            0.700000                9                        9  \n",
       "1            0.336364                6                        7  \n",
       "2            0.200000                9                        7  \n",
       "3            0.462500                6                        8  \n",
       "4            0.000000                5                        6  \n",
       "..                ...              ...                      ...  \n",
       "995          0.220833                7                        7  \n",
       "996          0.450000                8                        8  \n",
       "997          0.400000                8                        8  \n",
       "998          0.200000                7                        7  \n",
       "999          0.200000                7                        7  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the results we see the mean of every column in our data set using mean() fuction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = data_clean.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polarity                   0.279714\n",
       "stemmed_polarity           0.283104\n",
       "predicted_stars            7.102000\n",
       "stemmed_predicted_stars    7.066000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following steps we will run three more different datasets from other movies. We will also create a csv file to store the results of every run of each movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('ratings.csv', 'a+', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "  #  writer.writerow([Polarity\", \"Steammed Polarity \", \"predicted stars\", \"stemmed Predicted stars\"])\n",
    "    writer.writerow(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we store the results of different movies run throught the algorithm, we also hand-write the ratings from another website to compare our results. We will display our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show/ Movie</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Steammed Polarity</th>\n",
       "      <th>predicted stars</th>\n",
       "      <th>stemmed Predicted stars</th>\n",
       "      <th>Rooten tomatoe rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.308988</td>\n",
       "      <td>0.285046</td>\n",
       "      <td>7.274151</td>\n",
       "      <td>7.156658</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.273936</td>\n",
       "      <td>0.269426</td>\n",
       "      <td>7.082800</td>\n",
       "      <td>7.038700</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>0.356915</td>\n",
       "      <td>7.522300</td>\n",
       "      <td>7.463100</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.400423</td>\n",
       "      <td>0.371288</td>\n",
       "      <td>7.661240</td>\n",
       "      <td>7.531622</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.261314</td>\n",
       "      <td>0.273632</td>\n",
       "      <td>7.021918</td>\n",
       "      <td>7.043836</td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.279714</td>\n",
       "      <td>0.283104</td>\n",
       "      <td>7.102000</td>\n",
       "      <td>7.066000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show/ Movie  Polarity  Steammed Polarity   predicted stars  \\\n",
       "0     1.000000  0.308988            0.285046         7.274151   \n",
       "1     2.000000  0.273936            0.269426         7.082800   \n",
       "2     3.000000  0.370965            0.356915         7.522300   \n",
       "3     4.000000  0.400423            0.371288         7.661240   \n",
       "4     5.000000  0.261314            0.273632         7.021918   \n",
       "5     0.279714  0.283104            7.102000         7.066000   \n",
       "\n",
       "   stemmed Predicted stars  Rooten tomatoe rating  \n",
       "0                 7.156658                   0.91  \n",
       "1                 7.038700                   0.86  \n",
       "2                 7.463100                   0.68  \n",
       "3                 7.531622                   0.83  \n",
       "4                 7.043836                   0.81  \n",
       "5                      NaN                    NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingScore = pd.read_csv('ratings.csv')\n",
    "ratingScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this project we realized that \n",
    "\n",
    "\n",
    "1) people are generally a bit more possitive when leaving reviews of movies they did not hate meaning that if they generally thought the movie was okay they would rate it between 3-5, and people generally do not leave a 1 star review unless they absolutly hated the move.\n",
    "\n",
    "2) the language used in tweets is a bit harder for sentiment analysis to accuratly predict. The difference being that sometimes they talk negativly about a character in the show because of the actions that the character did, but the overall thought on the show could be positive but the rating predicted would be innacurate because we can't tell if they're talking about the show itself or just a character protraied in the show.\n",
    "\n",
    "3) The sentiment analysis was pretty accurate at detecting a negative vs a positive tweet regardless of the topic. Meaning we can use this method to analyize other data that we can collect from twitter data. \n",
    "\n",
    "So how usefull is this tool? I would say that given how much raw data is avaiable on twitter that this is a very powerful tool that can tell you if something (a product) is being portraied negativly or positivily online. With sentiment analysis we can figure out the following.\n",
    "\n",
    "1) What are people talking about the most (most commonly used words)\n",
    "\n",
    "2) What people hate about the product (polarity of -1 to -0.5)\n",
    "\n",
    "3) What people love about the product (polarity of 0.5 to 1)\n",
    "\n",
    "4) what people think that the product can improve on (-0.2 to 0.2 polarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
